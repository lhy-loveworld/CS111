NAME: Hongyang Li
EMAIL: hyli@g.ucla.edu
ID: 304759850

Cited Sources:
	Manual pages and TA's discussion slides
	http://stackoverflow.com/questions/7666509/hash-function-for-string
	
Answers to questions:
	QUESTION 2.3.1 - Cycles in the basic list implementation:
		Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?

			Most of the cycles are spent on list operations.

		Why do you believe these to be the most expensive parts of the code?

			Because there are only 1 or 2 threads, the contention is very low. The CPU can optimize it so that locks don't need to wait for too long.

		Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?

			Most of the time are spent on checking and spinning for locks.

		Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

			Most of the time are spent on list operations or context switches.

	QUESTION 2.3.2 - Execution Profiling:
		Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?

			The codes for checking and spinning for the locks.

		Why does this operation become so expensive with large numbers of threads?

			Because when the number of threads is large, there is a lot of contention when running the program. And spin-locks will spin rather than sleeping when waiting for a lock.

	QUESTION 2.3.3 - Mutex Wait Time:
		Look at the average time per operation (vs # threads) and the average wait-for-mutex time (vs #threads).
		Why does the average lock-wait time rise so dramatically with the number of contending threads?

			Because only one thread can get the lock at a time, when there are more threads, it cost more time to get the lock.

		Why does the completion time per operation rise (less dramatically) with the number of contending threads?

			Because some overhead time rises slowly or even stays the same when there are more threads, thus this part of time each operation shares drops, therefore the compeletion time per operation rise less dramatically.

		How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

			Because the completion time per operation is for a single operation, a thread may wait for multiple threads until it get the lock.

	QUESTION 2.3.4 - Performance of Partitioned Lists
		Explain the change in performance of the synchronized methods as a function of the number of lists.

			The throughput increases as the number of lists rises. More lists means each list is smaller. Thus the chance of contention drops.

		Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.

			It will not, because the number of threads the CPU can run at a time is not infinite.

		It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

			It is true. Both decreasing the threads and increasing the lists can increase the throughput.